import requests
from datetime import datetime
from openai import OpenAI
import time
import os
import json
from pathlib import Path
from dotenv import load_dotenv
import re
import base64

load_dotenv()
# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (API í‚¤ê°€ ìˆìœ¼ë©´ë§Œ ì‚¬ìš©)
try:
    if os.getenv("OPENAI_API_KEY"):
        client = OpenAI()
    else:
        client = None
except Exception as e:
    print(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    client = None

# ìºì‹œ íŒŒì¼ ê²½ë¡œ
CACHE_VERSION = 2
CACHE_DIR = Path(__file__).parent.parent.parent / ".cache"
CACHE_DIR.mkdir(exist_ok=True)
CACHE_FILE = CACHE_DIR / "news_cache.json"

# ë‰´ìŠ¤ ìºì‹œ (ë©”ëª¨ë¦¬ì— ì €ì¥)
_news_cache = {
    "data": None,
    "timestamp": None,
    "ttl": 86400  # 24ì‹œê°„
}

GENERATED_DIR = Path(__file__).parent.parent / "static" / "generated"
GENERATED_DIR.mkdir(parents=True, exist_ok=True)

def _pick_emoji(title: str) -> str:
    text = (title or "").lower()
    if any(k in text for k in ["ai", "artificial intelligence", "ml", "machine learning"]):
        return "ğŸ¤–"
    if any(k in text for k in ["cloud", "aws", "gcp", "azure", "datacenter"]):
        return "â˜ï¸"
    if any(k in text for k in ["kubernetes", "k8s"]):
        return "ğŸš¢"
    if "docker" in text:
        return "ğŸ³"
    if any(k in text for k in ["security", "vulnerability", "breach", "zero-day"]):
        return "ğŸ”’"
    if any(k in text for k in ["data", "database", "analytics", "warehouse"]):
        return "ğŸ“Š"
    if any(k in text for k in ["chip", "semiconductor", "gpu", "cpu"]):
        return "ğŸ§ "
    if any(k in text for k in ["robot", "automation"]):
        return "ğŸ¦¾"
    return "ğŸ“°"

def _build_image_prompt(title: str) -> str:
    text = (title or "").strip()
    return (
        "Create a clean, modern, editorial illustration for a tech news article. "
        "Style: flat illustration, minimal shapes, subtle gradients. "
        "No text, no logos, no brand marks. "
        "High-contrast, minimal, professional. "
        "Theme: " + text
    )

def _clean_summary(text: str) -> str:
    if not text:
        return ""
    # Remove markdown headers and bullets
    cleaned = re.sub(r"^#{1,6}\s*", "", text, flags=re.MULTILINE)
    cleaned = re.sub(r"^[\-\*\u2022]\s+", "", cleaned, flags=re.MULTILINE)
    cleaned = cleaned.replace("**", "").replace("__", "")
    # Collapse whitespace
    cleaned = re.sub(r"\n{2,}", "\n", cleaned).strip()
    return cleaned

def _parse_headline_and_summary(gpt_content: str, fallback_title: str):
    headline = fallback_title
    summary = ""
    detail_lines = []
    in_detail = False
    after_summary = False
    for line in gpt_content.splitlines():
        stripped = line.strip()
        if stripped.startswith("HEADLINE:"):
            headline = line.split("HEADLINE:", 1)[1].strip()
        elif stripped.startswith("SUMMARY:"):
            summary = line.split("SUMMARY:", 1)[1].strip()
            after_summary = True
        elif stripped.startswith("DETAIL:"):
            in_detail = True
            after_summary = False
            rest = line.split("DETAIL:", 1)[1].strip()
            if rest:
                detail_lines.append(rest)
        elif in_detail:
            detail_lines.append(line)
        elif after_summary:
            # If model omitted DETAIL:, treat remaining lines as detail
            detail_lines.append(line)
    summary = _clean_summary(summary)
    detail = "\n".join(detail_lines).strip()
    return headline or fallback_title, summary, detail

def _safe_image_name(raw_id: str) -> str:
    safe = re.sub(r"[^a-zA-Z0-9_-]", "_", str(raw_id))
    return f"news_{safe}.png"

def _generate_detail_markdown(title: str):
    if not client:
        return ""
    try:
        resp = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "ê¸°ìˆ  ê¸°ì‚¬ ì œëª©ì„ ë³´ê³  ì•„ë˜ í˜•ì‹ì˜ í•œêµ­ì–´ ìƒì„¸ ìš”ì•½ë§Œ ì‘ì„±í•˜ì„¸ìš”. "
                        "í˜•ì‹ê³¼ ì¤„ë°”ê¿ˆì„ ì§€ì¼œì•¼ í•©ë‹ˆë‹¤.\n"
                        "SUMMARY: [ê²°ë¡  í•œ ë¬¸ì¥]\n"
                        "## í•µì‹¬ ì£¼ì¥\n"
                        "- [ì£¼ì¥ 1]\n"
                        "- [ì£¼ì¥ 2]\n"
                        "- [ì£¼ì¥ 3]\n"
                        "## ì™œ ì¤‘ìš”í•œê°€\n"
                        "- [ì‹¤ë¬´/ìš´ì˜ ê´€ì  ì˜ë¯¸]\n"
                        "- [í˜„ì¬ ê¸°ìˆ  íë¦„ì—ì„œì˜ ì¤‘ìš”ì„±]"
                    )
                },
                {
                    "role": "user",
                    "content": f"ì œëª©: {title}"
                }
            ],
            temperature=0.7,
            max_tokens=800
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        print(f"DETAIL ìƒì„± ì‹¤íŒ¨: {e}")
        return ""

def _generate_gpt_image(prompt: str, raw_id: str):
    if not client:
        return None

    filename = _safe_image_name(raw_id)
    file_path = GENERATED_DIR / filename
    if file_path.exists():
        return {
            "image_url": f"/static/generated/{filename}",
            "image_alt": prompt
        }

    try:
        response = client.images.generate(
            model="gpt-image-1-mini",
            prompt=prompt,
            size="1536x1024",
            quality="low"
        )
        if not response or not getattr(response, "data", None):
            return None

        image_base64 = response.data[0].b64_json
        if not image_base64:
            return None

        file_path.write_bytes(base64.b64decode(image_base64))
        return {
            "image_url": f"/static/generated/{filename}",
            "image_alt": prompt
        }
    except Exception as e:
        print(f"GPT ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def _enrich_news_item(item: dict, title: str):
    item["emoji"] = _pick_emoji(title)
    prompt = _build_image_prompt(title)
    image_data = _generate_gpt_image(prompt, item.get("id"))
    if image_data and image_data.get("image_url"):
        item.update(image_data)
    return item

def _load_cache_from_file():
    """íŒŒì¼ì—ì„œ ìºì‹œ ë¡œë“œ"""
    if CACHE_FILE.exists():
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
                if cache_data.get("version") != CACHE_VERSION:
                    return False
                _news_cache["data"] = cache_data.get("data")
                _news_cache["timestamp"] = cache_data.get("timestamp")
                return _is_cache_valid()
        except Exception as e:
            print(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
    return False

def _save_cache_to_file():
    """íŒŒì¼ì— ìºì‹œ ì €ì¥"""
    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump({
                "version": CACHE_VERSION,
                "data": _news_cache["data"],
                "timestamp": _news_cache["timestamp"]
            }, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

def _is_cache_valid():
    """ìºì‹œê°€ ìœ íš¨í•œì§€ í™•ì¸"""
    if _news_cache["data"] is None or _news_cache["timestamp"] is None:
        return False
    return (time.time() - _news_cache["timestamp"]) < _news_cache["ttl"]

def get_tech_news():
    """
    HackerNews APIì—ì„œ ìµœì‹  ê¸°ìˆ  ë‰´ìŠ¤ 3ê°œë¥¼ ê°€ì ¸ì˜¤ê³ ,
    GPTë¥¼ ì‚¬ìš©í•´ ìê·¹ì ì¸ ì œëª©ê³¼ í•œê¸€ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    ë©”ëª¨ë¦¬ì™€ íŒŒì¼ ìºì‹±ì„ í†µí•´ ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤.
    """
    print("[INFO] get_tech_news() í˜¸ì¶œë¨")
    
    # ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
    if _is_cache_valid():
        print("[INFO] ë©”ëª¨ë¦¬ ìºì‹œì—ì„œ ë°˜í™˜")
        return _news_cache["data"]
    
    # íŒŒì¼ ìºì‹œ ë¡œë“œ ì‹œë„
    if _load_cache_from_file():
        print("[INFO] íŒŒì¼ ìºì‹œì—ì„œ ë¡œë“œë¨")
        return _news_cache["data"]
    
    print("[INFO] ìƒˆë¡œìš´ ë‰´ìŠ¤ ë°ì´í„° ìƒì„± ì¤‘...")
    try:
        # HackerNews APIì—ì„œ ìƒìœ„ ìŠ¤í† ë¦¬ ID ê°€ì ¸ì˜¤ê¸°
        top_stories_url = "https://hacker-news.firebaseio.com/v0/topstories.json"
        response = requests.get(top_stories_url, timeout=5)
        top_story_ids = response.json()[:5]  # ìƒìœ„ 15ê°œ ê°€ì ¸ì˜¤ê¸°
        
        news_items = []
        story_url = "https://hacker-news.firebaseio.com/v0/item/{}.json"
        
        for story_id in top_story_ids:
            try:
                item_response = requests.get(story_url.format(story_id), timeout=3)
                item = item_response.json()
                
                # í•„ìš”í•œ í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                if "title" in item and "url" in item:
                    original_title = item.get("title", "")
                    
                    # GPTë¥¼ ì‚¬ìš©í•´ ìê·¹ì ì¸ ì œëª©ê³¼ í•œê¸€ ìš”ì•½ ìƒì„± (API í‚¤ê°€ ìˆì„ ë•Œë§Œ)
                    try:
                        if client:
                            gpt_response = client.chat.completions.create(
                                model="gpt-3.5-turbo",
                                messages=[
                                    {
                                        "role": "system",
                                        "content": (
                                            "ë‹¹ì‹ ì€ 'ê°í…Œí¬' YouTube ì±„ë„ì˜ ì‹œë‹ˆì–´ ê¸°ìˆ  ì—ë””í„°ì…ë‹ˆë‹¤. "
                                            "20-30ëŒ€ ê°œë°œìì™€ í•™ìƒë“¤ì´ 'ì˜¤, ì´ê±° ë´ì•¼ê² ë‹¤!'ë¼ê³  ìƒê°í•˜ê²Œ ë§Œë“œëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.\n\n"
                                            
                                            "## í—¤ë“œë¼ì¸ ì‘ì„± ì›ì¹™\n"
                                            "- ê¸°ìˆ  íŠ¸ë Œë“œì˜ 'ì§„ì§œ ì˜ë¯¸'ë¥¼ ì§šì–´ë‚´ì„¸ìš” (ì˜ˆ: 'ì´ê±° ì•ˆ ì“°ë©´ ë’¤ì²˜ì§„ë‹¤', 'ì—…ê³„ íŒë„ê°€ ë°”ë€ë‹¤')\n"
                                            "- êµ¬ì²´ì  ìˆ«ìë‚˜ ì„íŒ©íŠ¸ë¥¼ ë„£ìœ¼ì„¸ìš” (ì˜ˆ: 'ì„±ëŠ¥ 3ë°°', 'ê°œë°œ ì‹œê°„ 50% ë‹¨ì¶•')\n"
                                            "- ì‹¤ë¬´ìì˜ ê³ ë¯¼ì„ ê±´ë“œë¦¬ì„¸ìš” (ì˜ˆ: 'ë©´ì ‘ì—ì„œ ë¬¼ì–´ë³¸ë‹¤', 'ì´ì œ legacy ëœë‹¤')\n"
                                            "- ì´ëª¨ì§€ 1-2ê°œë¡œ ì‹œì„  ì§‘ì¤‘ (ğŸš€âš¡ğŸ”¥ğŸ’€ğŸ¯ğŸ¤¯)\n\n"
                                            
                                            "## ìš”ì•½ ì‘ì„± ì›ì¹™\n"
                                            "- ì²« ë¬¸ì¥: í•µì‹¬ ê²°ë¡ ì„ ë‹¨ì •ì ìœ¼ë¡œ ('~ì…ë‹ˆë‹¤', '~ë©ë‹ˆë‹¤')\n"
                                            "- ë‘ ë²ˆì§¸: ì™œ ì§€ê¸ˆ ì¤‘ìš”í•œì§€ ì‹¤ë¬´ ë§¥ë½ ('í˜„ì—…ì—ì„œëŠ”~', 'ì´ë¯¸ ëŒ€ê¸°ì—…ë“¤ì€~')\n"
                                            "- ì„¸ ë²ˆì§¸: ë…ìê°€ ì·¨í•  ì•¡ì…˜ íŒíŠ¸ ('ì£¼ëª©í•´ì•¼ í•  ì´ìœ ëŠ”~', 'ë°”ë€ŒëŠ” ê±´~')\n"
                                            "- ë§ˆí¬ë‹¤ìš´ ì—†ì´ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ 2-3ë¬¸ì¥\n\n"
                                            
                                            "## ìƒì„¸ ë‚´ìš© êµ¬ì¡°\n"
                                            "DETAIL: ë’¤ì—ëŠ” ë°˜ë“œì‹œ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì•„ë˜ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”:\n\n"
                                            "SUMMARY: [í•œ ì¤„ë¡œ í•µì‹¬ ì •ë¦¬ - ê°•ë ¬í•˜ê²Œ]\n\n"
                                            "## ğŸ¯ í•µì‹¬ í¬ì¸íŠ¸\n"
                                            "- [êµ¬ì²´ì  ë³€í™”/ìˆ˜ì¹˜/ì‚¬ë¡€ 1]\n"
                                            "- [ì‹¤ë¬´ ì˜í–¥ 2]\n"
                                            "- [ê¸°ìˆ ì  ì˜ì˜ 3]\n\n"
                                            "## ğŸ’¡ ì™œ ì§€ê¸ˆ ì£¼ëª©í•´ì•¼ í•˜ë‚˜\n"
                                            "- [í˜„ì—… ê´€ì : ì±„ìš©/ë©´ì ‘/í”„ë¡œì íŠ¸ì—ì„œ ì–´ë–»ê²Œ ì“°ì´ëŠ”ê°€]\n"
                                            "- [ê¸°ìˆ  íŠ¸ë Œë“œ: ì—…ê³„ê°€ ì–´ë””ë¡œ ê°€ê³  ìˆëŠ”ê°€]\n"
                                            "- [ëŸ¬ë‹ í¬ì¸íŠ¸: ê°œë°œìê°€ ë°°ì›Œì•¼ í•  ê²ƒ]\n\n"
                                            
                                            "## ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ ì§€í‚¬ ê²ƒ)\n"
                                            "HEADLINE: [ìê·¹ì ì´ê³  êµ¬ì²´ì ì¸ í—¤ë“œë¼ì¸]\n"
                                            "SUMMARY: [2-3ë¬¸ì¥ì˜ ëª…í™•í•œ í•œêµ­ì–´ ìš”ì•½, ë§ˆí¬ë‹¤ìš´ ì—†ìŒ]\n"
                                            "DETAIL:\n"
                                            "SUMMARY: [í•œ ì¤„ í•µì‹¬]\n"
                                            "## ğŸ¯ í•µì‹¬ í¬ì¸íŠ¸\n...\n"
                                            "## ğŸ’¡ ì™œ ì§€ê¸ˆ ì£¼ëª©í•´ì•¼ í•˜ë‚˜\n...\n\n"
                                            
                                            "ì˜ˆì‹œ í†¤:\n"
                                            "âŒ ë‚˜ìœ ì˜ˆ: 'Kubernetes 1.30ì´ ì¶œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.'\n"
                                            "âœ… ì¢‹ì€ ì˜ˆ: 'ğŸš€ ì¿ ë²„ë„¤í‹°ìŠ¤ 1.30 ì¶©ê²©! ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 40% ê°ì†Œ, ì´ì œ ì¤‘ì†Œê¸°ì—…ë„ ì“´ë‹¤'\n\n"
                                            
                                            "âŒ ë‚˜ìœ ì˜ˆ: 'AI ëª¨ë¸ì´ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.'\n"
                                            "âœ… ì¢‹ì€ ì˜ˆ: 'ğŸ¤– GPT-5 ì‹¤í™”ëƒ? ì½”ë”© í…ŒìŠ¤íŠ¸ ë§Œì , ì‹œë‹ˆì–´ ê°œë°œì ìœ„ê¸°ì„¤'\n\n"
                                            
                                            "ê¸°ì–µí•˜ì„¸ìš”: ë…ìëŠ” ë°”ìœ í˜„ì—… ê°œë°œìì…ë‹ˆë‹¤. "
                                            "3ì´ˆ ì•ˆì— 'ì•„, ì´ê±° ë‚´ê°€ ì•Œì•„ì•¼ í•˜ëŠ” ê±°ë„¤' ëŠë¼ê²Œ ë§Œë“œì„¸ìš”!"
                                        )
                                    },
                                    {
                                        "role": "user",
                                        "content": (
                                            f"ë‹¤ìŒ ì˜ì–´ ê¸°ìˆ  ê¸€ ì œëª©ì„ ê¸°ë°˜ìœ¼ë¡œ "
                                            f"ìê·¹ì ì¸ í—¤ë“œë¼ì¸ê³¼ í•œêµ­ì–´ ìš”ì•½ì„ ì‘ì„±í•´ì¤˜.\n\n"
                                            f"ì›ë¬¸ ì œëª©:\n{original_title}"
                                        )
                                    }
                                ],
                                temperature=0.7,
                                max_tokens=2500
                            )
                            
                            gpt_content = gpt_response.choices[0].message.content.strip()
                            print(f"[DEBUG] GPT ì‘ë‹µ: {gpt_content}")
                            
                            # GPT ì‘ë‹µ íŒŒì‹±
                            new_title, description, detail_markdown = _parse_headline_and_summary(
                                gpt_content, original_title
                            )
                            
                            # detail_markdownì´ ì—†ìœ¼ë©´ ì¶”ê°€ ìƒì„±
                            if not detail_markdown:
                                detail_markdown = _generate_detail_markdown(original_title)
                            
                            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í´ë°±
                            if not description:
                                description = f"Posted by {item.get('by', 'Anonymous')} with {item.get('score', 0)} points"
                        else:
                            # API í‚¤ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
                            print("[DEBUG] OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤")
                            new_title = original_title
                            description = f"Posted by {item.get('by', 'Anonymous')} with {item.get('score', 0)} points"
                            detail_markdown = ""
                        
                        # ìƒˆ ì•„ì´í…œ ìƒì„± (ëª¨ë“  í•„ë“œ í¬í•¨)
                        new_item = {
                            "id": story_id,
                            "title": new_title,
                            "original_title": original_title,
                            "source": "HackerNews",
                            "url": item.get("url", ""),
                            "date": datetime.now().strftime("%Y-%m-%d"),
                            "description": description,
                            "summary": description,  # summary í•„ë“œ ì¶”ê°€
                            "short_description": description[:160] if description else "",  # short_description í•„ë“œ ì¶”ê°€
                            "detail_markdown": detail_markdown if detail_markdown else "",  # detail_markdown í•„ë“œ ì¶”ê°€
                            "score": item.get("score", 0)
                        }
                        
                        # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•˜ê³  ì´ë¯¸ì§€/ì´ëª¨ì§€ enrichment
                        news_items.append(new_item)
                        _enrich_news_item(news_items[-1], original_title or new_title)
                        
                        if len(news_items) >= 3:
                            break
                            
                    except Exception as e:
                        # GPT í˜¸ì¶œ ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°ì´í„° ì‚¬ìš©
                        print(f"GPT ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        new_item = {
                            "id": story_id,
                            "title": original_title,
                            "original_title": original_title,
                            "source": "HackerNews",
                            "url": item.get("url", ""),
                            "date": datetime.now().strftime("%Y-%m-%d"),
                            "description": f"Posted by {item.get('by', 'Anonymous')} with {item.get('score', 0)} points",
                            "summary": f"Posted by {item.get('by', 'Anonymous')} with {item.get('score', 0)} points",
                            "short_description": f"Posted by {item.get('by', 'Anonymous')}",
                            "detail_markdown": "",
                            "score": item.get("score", 0)
                        }
                        
                        news_items.append(new_item)
                        _enrich_news_item(news_items[-1], original_title)
                        
                        if len(news_items) >= 3:
                            break
            except:
                continue
        
        result = news_items if news_items else get_fallback_news()
        
        # ìºì‹œì— ì €ì¥ (ë©”ëª¨ë¦¬ + íŒŒì¼)
        _news_cache["data"] = result
        _news_cache["timestamp"] = time.time()
        _save_cache_to_file()
        
        return result
    except:
        # ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•œ ê²½ìš° ê¸°ë³¸ ë‰´ìŠ¤ ë°˜í™˜
        result = get_fallback_news()
        
        # í´ë°± ë‰´ìŠ¤ë„ ìºì‹œ (ë©”ëª¨ë¦¬ + íŒŒì¼)
        _news_cache["data"] = result
        _news_cache["timestamp"] = time.time()
        _save_cache_to_file()
        
        return result

def get_fallback_news():
    """
    API ìš”ì²­ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë‰´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    items = [
        {
            "id": "fallback_1",
            "title": "ğŸš€ ì¿ ë²„ë„¤í‹°ìŠ¤ 1.29 ì¶œì‹œ! ì„±ëŠ¥ í˜ì‹ ì˜ ìƒˆë¡œìš´ ì‹œëŒ€",
            "original_title": "Kubernetes 1.29 Release",
            "source": "Kubernetes Blog",
            "url": "https://kubernetes.io/blog/",
            "date": datetime.now().strftime("%Y-%m-%d"),
            "description": "ìµœì‹  ë²„ì „ì—ì„œ ì„±ëŠ¥ì´ ê°œì„ ë˜ì—ˆê³  ìƒˆë¡œìš´ APIê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì˜ ë¯¸ë˜ë¥¼ ë§Œë‚˜ë³´ì„¸ìš”.",
            "summary": "ìµœì‹  ë²„ì „ì—ì„œ ì„±ëŠ¥ì´ ê°œì„ ë˜ì—ˆê³  ìƒˆë¡œìš´ APIê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì˜ ë¯¸ë˜ë¥¼ ë§Œë‚˜ë³´ì„¸ìš”.",
            "short_description": "ìµœì‹  ë²„ì „ì—ì„œ ì„±ëŠ¥ì´ ê°œì„ ë˜ì—ˆê³  ìƒˆë¡œìš´ APIê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "detail_markdown": "## í•µì‹¬ ì£¼ì¥\n- ì„±ëŠ¥ ê°œì„ \n- ìƒˆë¡œìš´ API ì¶”ê°€\n\n## ì™œ ì¤‘ìš”í•œê°€\n- ì»¨í…Œì´ë„ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì˜ ë°œì „",
            "score": 100
        },
        {
            "id": "fallback_2",
            "title": "ğŸ¤– Docker Desktopì— AI ê¸°ëŠ¥ í†µí•©! ê°œë°œ ìƒì‚°ì„± í­ì¦",
            "original_title": "Docker Desktop AI Integration",
            "source": "Docker",
            "url": "https://www.docker.com/blog/",
            "date": datetime.now().strftime("%Y-%m-%d"),
            "description": "Docker Desktopì— AI ê¸°ë°˜ì˜ ì´ë¯¸ì§€ ë¶„ì„ ê¸°ëŠ¥ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ë” ë˜‘ë˜‘í•œ ì»¨í…Œì´ë„ˆ ê´€ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
            "summary": "Docker Desktopì— AI ê¸°ë°˜ì˜ ì´ë¯¸ì§€ ë¶„ì„ ê¸°ëŠ¥ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ë” ë˜‘ë˜‘í•œ ì»¨í…Œì´ë„ˆ ê´€ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
            "short_description": "Docker Desktopì— AI ê¸°ë°˜ì˜ ì´ë¯¸ì§€ ë¶„ì„ ê¸°ëŠ¥ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "detail_markdown": "## í•µì‹¬ ì£¼ì¥\n- AI ê¸°ëŠ¥ í†µí•©\n- ìƒì‚°ì„± í–¥ìƒ\n\n## ì™œ ì¤‘ìš”í•œê°€\n- ê°œë°œ ì›Œí¬í”Œë¡œìš° ê°œì„ ",
            "score": 85
        },
        {
            "id": "fallback_3",
            "title": "âš¡ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ê°€ ì—”í„°í”„ë¼ì´ì¦ˆë¥¼ ì œì••",
            "original_title": "Microservices Architecture Enterprise Trend",
            "source": "DevOps Digest",
            "url": "https://devops.com/",
            "date": datetime.now().strftime("%Y-%m-%d"),
            "description": "ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ê¸°ë°˜ ì•„í‚¤í…ì²˜ê°€ ì—”í„°í”„ë¼ì´ì¦ˆ í™˜ê²½ì—ì„œ ì£¼ë¥˜ê°€ ë˜ê³  ìˆìŠµë‹ˆë‹¤. ë¶„ì‚° ì‹œìŠ¤í…œì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì…ë‹ˆë‹¤.",
            "summary": "ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ê¸°ë°˜ ì•„í‚¤í…ì²˜ê°€ ì—”í„°í”„ë¼ì´ì¦ˆ í™˜ê²½ì—ì„œ ì£¼ë¥˜ê°€ ë˜ê³  ìˆìŠµë‹ˆë‹¤. ë¶„ì‚° ì‹œìŠ¤í…œì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì…ë‹ˆë‹¤.",
            "short_description": "ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ê¸°ë°˜ ì•„í‚¤í…ì²˜ê°€ ì—”í„°í”„ë¼ì´ì¦ˆ í™˜ê²½ì—ì„œ ì£¼ë¥˜ê°€ ë˜ê³  ìˆìŠµë‹ˆë‹¤.",
            "detail_markdown": "## í•µì‹¬ ì£¼ì¥\n- ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ ì±„íƒ ì¦ê°€\n- ì—”í„°í”„ë¼ì´ì¦ˆ í™˜ê²½ ì ìš©\n\n## ì™œ ì¤‘ìš”í•œê°€\n- ë¶„ì‚° ì‹œìŠ¤í…œì˜ ë¯¸ë˜",
            "score": 72
        }
    ]
    for item in items:
        _enrich_news_item(item, item.get("original_title") or item.get("title"))
    return items